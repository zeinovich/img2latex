{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 3x3 conv with padding size 1 (to leave the input size unchanged), followed by a ReLU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        padding: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        )\n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        c = self.conv(x)\n",
    "        r = self.relu(c)\n",
    "        return r\n",
    "\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim: int = 80,\n",
    "        hidden_dim: int = 512,\n",
    "        dropout_prob: float = 0,\n",
    "        vocab_len: int = None,\n",
    "        max_output_length: int = 512,\n",
    "        device: str = \"cpu\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # At least special tokens must be present\n",
    "        assert vocab_len > 4, \"Vocabulary length must be at least 4\"\n",
    "\n",
    "        self._encoder_out = hidden_dim\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._vocab_len = vocab_len\n",
    "        # standard vocabulary with special tokens\n",
    "        self._vocab = {\n",
    "            \"<UNK>\": 0,\n",
    "            \"<SOS>\": 1,\n",
    "            \"<PAD>\": 2,\n",
    "            \"<EOS>\": 3,\n",
    "        }\n",
    "        # output dim is set to vocab_len\n",
    "        self._output_dim = self._vocab_len\n",
    "        self._emb_dim = emb_dim\n",
    "        self._max_len = max_output_length\n",
    "        self._device = device\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            ConvBlock(in_channels=1, out_channels=64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            ConvBlock(in_channels=64, out_channels=128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            ConvBlock(in_channels=128, out_channels=256),\n",
    "            ConvBlock(in_channels=256, out_channels=256),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),\n",
    "            ConvBlock(\n",
    "                in_channels=256, out_channels=self._encoder_out, padding=0\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=self._emb_dim + self._encoder_out,\n",
    "            hidden_size=self._hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.embedding = nn.Embedding(\n",
    "            self._output_dim,\n",
    "            self._emb_dim,  # padding_idx=self._vocab[\"<PAD>\"]\n",
    "        )\n",
    "        self.hidden0_fc = nn.Sequential(\n",
    "            nn.Linear(self._encoder_out, self._hidden_dim)\n",
    "        )\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(self._hidden_dim, self._output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # INITIALIZE OUTPUT TENSORS\n",
    "        # outputs of shape (B, MAX_LEN, VOCAB_SIZE (OUTPUT_DIM))\n",
    "        outputs = (\n",
    "            torch.ones(\n",
    "                batch_size, self._max_len, self._output_dim, requires_grad=True\n",
    "            )\n",
    "            .type_as(x)\n",
    "            .to(self._device)\n",
    "            * self._vocab[\"<PAD>\"]\n",
    "        )\n",
    "        # 1st input is always <START_SEQ>\n",
    "        input_token = (\n",
    "            torch.ones(batch_size, 1).type_as(x).to(self._device).long()\n",
    "            * self._vocab[\"<SOS>\"]\n",
    "        )\n",
    "\n",
    "        # ENCODE IMAGE\n",
    "        encoded_img = self.encoder(x)\n",
    "        encoded_img = encoded_img.permute(\n",
    "            0, 2, 3, 1\n",
    "        )  # make B * H * W * HIDDEN_DIM to use contiguous\n",
    "\n",
    "        _, H, W, _ = encoded_img.size()\n",
    "\n",
    "        encoded_img = encoded_img.contiguous().view(\n",
    "            batch_size,\n",
    "            H * W,\n",
    "            self._encoder_out,\n",
    "        )  # [B, HIDDEN_DIM, H * W]\n",
    "        encoded_img = encoded_img.mean(dim=1)  # [B, HIDDEN_DIM]\n",
    "\n",
    "        hidden = self.hidden0_fc(encoded_img)\n",
    "        cell = torch.zeros(batch_size, self._hidden_dim)\n",
    "        output = torch.zeros(batch_size, self._hidden_dim)\n",
    "\n",
    "        for t in range(1, self._max_len):\n",
    "            hidden, cell, output = self.decode(\n",
    "                hidden=hidden,\n",
    "                cell=cell,\n",
    "                out_t=output,\n",
    "                input_token=input_token,\n",
    "            )\n",
    "\n",
    "            logit = self.fc_out(output)\n",
    "            outputs[:, t, ...] = logit\n",
    "            input_token = torch.argmax(logit, 1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        hidden: tuple[torch.Tensor],\n",
    "        cell: torch.Tensor,\n",
    "        out_t: torch.Tensor,\n",
    "        input_token: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor]:\n",
    "        prev_y = self.embedding(input_token).squeeze(1)\n",
    "        input_t = torch.cat([prev_y, out_t], 1)\n",
    "        out_t, (hidden_t, cell_t) = self.decoder(input_t, (hidden, cell))\n",
    "\n",
    "        return hidden_t, cell_t, out_t\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "        encoded_img = self.encoder(x)\n",
    "\n",
    "        hidden = encoded_img\n",
    "        cell = torch.zeros(batch_size, self._hidden_dim)\n",
    "        output = torch.zeros(batch_size, self._hidden_dim)\n",
    "\n",
    "        # outputs of shape (B, MAX_LEN, VOCAB_LEN (OUTPUT_DIM))\n",
    "        outputs = (\n",
    "            torch.ones(batch_size, self._max_len, self._output_dim)\n",
    "            .type_as(x)\n",
    "            .long()\n",
    "            .to(self._device)\n",
    "            * self._vocab[\"<PAD>\"]\n",
    "        )\n",
    "        # 1st input is always <START_SEQ>\n",
    "        input_token = (\n",
    "            torch.ones(batch_size, 1).type_as(x).to(self._device).long()\n",
    "            * self._vocab[\"<SOS>\"]\n",
    "        )\n",
    "\n",
    "        for t in range(1, self._max_len):\n",
    "            hidden, cell, output, logit = self.decode(\n",
    "                hidden=hidden,\n",
    "                cell=cell,\n",
    "                out_t=output,\n",
    "                input_token=input_token,\n",
    "            )\n",
    "\n",
    "            outputs[:, t, ...] = logit\n",
    "            input_token = torch.argmax(logit, 1)\n",
    "\n",
    "            if input_token == self._vocab[\"<EOS>\"]:\n",
    "                break\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(x)\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self._output_dim\n",
    "\n",
    "    @property\n",
    "    def max_len(self):\n",
    "        return self._max_len\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._device\n",
    "\n",
    "    @property\n",
    "    def img_size(self):\n",
    "        return self._img_size\n",
    "\n",
    "    @property\n",
    "    def img_size(self):\n",
    "        return self._img_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaTEXTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer for LaTEX exspressions. Vocabulary is generated from IM2LATEX-100k\n",
    "\n",
    "    ---\n",
    "    Parameters\n",
    "    ---\n",
    "    id2token: dict[str. int]\n",
    "        Mapping from tokens to indices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token2id: dict[str, int]) -> None:\n",
    "        self._token2id = {k: int(v) for k, v in token2id.items()}\n",
    "        self._id2token = {int(v): k for k, v in token2id.items()}\n",
    "\n",
    "    def tokenize(\n",
    "        self,\n",
    "        x: list[str],\n",
    "        return_tensors: bool = True,\n",
    "        pad: bool = True,\n",
    "        max_len: int = 512,\n",
    "    ) -> Union[torch.Tensor, list[list[int]]]:  # separate dots\n",
    "        \"\"\"\n",
    "        Tokenize list of sentences.\n",
    "\n",
    "        -----\n",
    "        Parameters\n",
    "        -----\n",
    "\n",
    "        x: list[str]\n",
    "            Input list of sentences\n",
    "\n",
    "        ---\n",
    "        Returns\n",
    "        ---\n",
    "        torch.Tensor\n",
    "            Tensor of indices with shape (B, MAX_LEN)\"\"\"\n",
    "\n",
    "        x = [s.replace(\".\", \" . \") for s in x]\n",
    "        # separate digits\n",
    "        x = [re.sub(r\"(\\d)\", r\" \\1\", s) for s in x]\n",
    "        x = [s.strip().split() for s in x]\n",
    "        x = [\n",
    "            [self._token2id.get(token, self._token2id[\"<UNK>\"]) for token in s]\n",
    "            for s in x\n",
    "        ]\n",
    "\n",
    "        if any(self._token2id[\"<UNK>\"] in s for s in x):\n",
    "            warnings.warn(\n",
    "                \"Got unknown token. May affect final result\",\n",
    "            )\n",
    "\n",
    "        # insert start and end tokens\n",
    "        x = [[self._token2id[\"<SOS>\"]] + s for s in x]\n",
    "        x = [s + [self._token2id[\"<EOS>\"]] for s in x]\n",
    "        x = [s[:max_len] for s in x]\n",
    "\n",
    "        if pad:\n",
    "            # pad sequences to max length\n",
    "            x = [s + [self._token2id[\"<PAD>\"]] * (max_len - len(s)) for s in x]\n",
    "\n",
    "        if return_tensors:\n",
    "            x = torch.Tensor(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        x: Union[torch.Tensor, np.ndarray, list[int]],\n",
    "    ) -> list[str]:\n",
    "        \"\"\"\n",
    "        Decodes input array of indices into a string(s)\n",
    "\n",
    "        ---\n",
    "        Parameters\n",
    "        ---\n",
    "        x: Union[torch.Tensor, np.ndarray, list[int]]\n",
    "            Input sequence of sequences of indices\n",
    "\n",
    "        ---\n",
    "        Returns\n",
    "        ---\n",
    "        list[str]\n",
    "            List of decoded strings (ready to be displayed as LaTEX)\n",
    "        \"\"\"\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = x.detach().cpu().tolist()\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = x.tolist()\n",
    "\n",
    "        x = [\n",
    "            [\n",
    "                self._id2token[i]\n",
    "                for i in s\n",
    "                if i not in list(self.special_tokens.values())\n",
    "                # ignore special tokens\n",
    "            ]\n",
    "            for s in x\n",
    "        ]\n",
    "        x = [\" \".join(s) for s in x]\n",
    "        x = [re.sub(r\" +\", \" \", s) for s in x]\n",
    "        x = [s.strip() for s in x]\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def special_tokens(self) -> dict[str, int]:\n",
    "        \"\"\"\n",
    "        Get mapping (str2int) of special tokens\n",
    "        \"\"\"\n",
    "        tokens = [\"<SOS>\", \"<PAD>\", \"<EOS>\"]\n",
    "        return {k: self.token2id.get(k, 3) for k in tokens}\n",
    "\n",
    "    @property\n",
    "    def id2token(self) -> dict[int, str]:\n",
    "        \"\"\"\n",
    "        Get vocabulary (int2str)\n",
    "        \"\"\"\n",
    "        return self._id2token\n",
    "\n",
    "    @property\n",
    "    def token2id(self) -> dict[str, int]:\n",
    "        \"\"\"\n",
    "        Get vocabulary (str2int)\n",
    "        \"\"\"\n",
    "        return self._token2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(path: str, col_name: str = None) -> pd.DataFrame:\n",
    "    # read input file or raise ValueError\n",
    "    ext = path.strip().split(\".\")[-1]\n",
    "\n",
    "    if ext == \"json\":\n",
    "        with open(path, \"r\") as f:\n",
    "            df = json.load(f)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            read_file = getattr(pd, f\"read_{ext}\")\n",
    "            df = read_file(path)\n",
    "            print(f\"{df.shape=}\")\n",
    "            # check if df has only one column\n",
    "            if not col_name:\n",
    "                assert (\n",
    "                    df.shape[1] == 1\n",
    "                ), \"DataFrame should have only one column or column to be used must be specified\"\n",
    "                col = df.columns.to_list()[0]\n",
    "                df = df.rename({col: \"formula\"}, axis=1)\n",
    "            # or name is specified\n",
    "            else:\n",
    "                df = df.rename({col_name: \"formula\"}, axis=1)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_output(path: str, obj: object) -> None:\n",
    "    if path.endswith(\".json\"):\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(obj, f, indent=4)\n",
    "    else:\n",
    "        ext = path.strip().split(\".\")[-1]\n",
    "\n",
    "        # get pandas to_{extension}, ie to_csv, method\n",
    "        save = getattr(obj, f\"to_{ext}\")\n",
    "        save(path, index=False)\n",
    "\n",
    "\n",
    "def prepare_csv_array(row: str) -> list[int]:\n",
    "    row = row.replace(\"[\", \"\")\n",
    "    row = row.replace(\"]\", \"\")\n",
    "    row = row.strip().split(\",\")\n",
    "    row = list(map(int, row))\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IM2LaTEX100K(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        image_folder: str,\n",
    "        vocab_len: int,\n",
    "        transform: Union[T.Compose, None],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._file = \"\"\n",
    "        self._data = data\n",
    "        self._transform = transform\n",
    "        self._vocab_len = vocab_len\n",
    "        self._image_folder = image_folder\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.Tensor]:\n",
    "        row = self._data.iloc[index]\n",
    "        tokens = row[\"formula\"]\n",
    "        img_path = f\"{self._image_folder}/{row['image']}\"\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        if self._transform is not None:\n",
    "            img = self._transform(img)\n",
    "\n",
    "        tokens = torch.Tensor(tokens[0]).long()\n",
    "\n",
    "        return img, tokens\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._data.shape[0]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"IM2LaTEXDataset(file={self._file}, image_folder={self._image_folder})\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab_func(\n",
    "    input_file: str, col_name: str, add_special: bool, # output_folder: str\n",
    ") -> None:\n",
    "    df = read_input(input_file, col_name)\n",
    "\n",
    "    df[\"formula\"] = df[\"formula\"].apply(lambda x: x.replace(\".\", \" . \"))\n",
    "    df[\"formula\"] = df[\"formula\"].apply(lambda x: re.sub(\"(\\d)\", r\" \\1\", x))\n",
    "    df[\"formula_tokenized\"] = df[\"formula\"].apply(lambda x: x.strip().split())\n",
    "\n",
    "    words = df[\"formula_tokenized\"].tolist()\n",
    "    vocab = Counter([x for sublist in words for x in sublist])\n",
    "    vocab = sorted(list(vocab.keys()))\n",
    "\n",
    "    if add_special:\n",
    "        vocab = [\"<UNK>\", \"<SOS>\", \"<PAD>\", \"<EOS>\"] + vocab\n",
    "\n",
    "    token2id = {k: i for i, k in enumerate(vocab)}\n",
    "    id2token = {i: k for i, k in enumerate(vocab)}\n",
    "\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(\n",
    "    input_file: str,\n",
    "    vocab: dict[str, int],\n",
    "    col_name: str,\n",
    "    add_padding: str,\n",
    "    # output_file: str,\n",
    "    debug: bool,\n",
    ") -> None:\n",
    "    token2id = vocab\n",
    "    df = read_input(input_file, col_name=col_name)\n",
    "    df_len = df.shape[0]\n",
    "    df = df.dropna(axis=0)\n",
    "\n",
    "    if df_len != df.shape[0]:\n",
    "        print(f\"Dropped {df_len - df.shape[0]} rows from {input_file}\")\n",
    "\n",
    "    if debug:\n",
    "        df = df.sample(1)\n",
    "\n",
    "    df[\"formula\"] = df[\"formula\"].apply(lambda x: [x])\n",
    "\n",
    "    tokenizer = LaTEXTokenizer(token2id=token2id)\n",
    "    df[\"tokenized_formula\"] = df[\"formula\"].apply(\n",
    "        tokenizer.tokenize, return_tensors=False, pad=add_padding, max_len=512\n",
    "    )\n",
    "    df = df.drop(\"formula\", axis=1)\n",
    "    df = df.rename({\"tokenized_formula\": \"formula\"}, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(token1: torch.Tensor, token2: torch.Tensor) -> int:\n",
    "    len1 = token1.size(0)\n",
    "    distances = torch.zeros((len1 + 1, len1 + 1))\n",
    "\n",
    "    distances[:, 0] = torch.arange(len1 + 1)\n",
    "    distances[0, :] = torch.arange(len1 + 1)\n",
    "\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "\n",
    "    for t1 in range(1, len1 + 1):\n",
    "        for t2 in range(1, len1 + 1):\n",
    "            if token1[t1 - 1] == token2[t2 - 1]:\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "\n",
    "                if a <= b and a <= c:\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif b <= a and b <= c:\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    return distances[len1][len1].item()\n",
    "\n",
    "\n",
    "def make_training_report(\n",
    "    loss: float, levenstein: float, accuracy: float, lr=float\n",
    ") -> str:\n",
    "    return f\"Loss={loss:.4f} | Acc={accuracy:.4f} | ED={levenstein:.1f} | LR={lr:.1e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_func(\n",
    "    model: nn.Module,\n",
    "    n_epochs: int,\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    scheduler: optim.lr_scheduler.LRScheduler,\n",
    "    dataloaders: dict[str, DataLoader],\n",
    "    checkpoint: str,\n",
    "    end_token: int,\n",
    ") -> tuple[dict[dict[int]], nn.Module]:\n",
    "    # training consists of 3 phases: train, val, test\n",
    "    # test is performed only once after model has been\n",
    "    # trained for NUM_EPOCHS\n",
    "\n",
    "    lengths = {phase: len(dl) for phase, dl in dataloaders.items()}\n",
    "    history = {}\n",
    "\n",
    "    # for checkpoints\n",
    "    keep = 2  # number of checkpoints to keep\n",
    "    _last_saved = []  # cached paths of checkpoints\n",
    "    # range (1, n_epochs + 1) for prettier monitoring\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        losses = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "        num_correct = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "        num_total = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "        accuracies = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "        total_distances = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "        levenstein = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "        min_loss = float(\"inf\")\n",
    "\n",
    "        for phase in [\"train\", \"val\", \"test\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                # eval on val and test\n",
    "                model.eval()\n",
    "\n",
    "            if phase == \"test\" and epoch != n_epochs:\n",
    "                # skip test phase if not last epoch\n",
    "                continue\n",
    "\n",
    "            # tqdm progress bar\n",
    "            with tqdm(\n",
    "                dataloaders[phase], miniters=1, unit=\"batch\"\n",
    "            ) as pbar, torch.set_grad_enabled(phase == \"train\"):\n",
    "                # if phase != \"train\" works as torch.no_grad()\n",
    "                for img, tokens in pbar:\n",
    "                    pbar.set_description_str(\n",
    "                        f\"{phase.capitalize():5}({epoch:03d})\"\n",
    "                    )\n",
    "\n",
    "                    # tokens [batch_size, max_len]\n",
    "                    # output [batch_size, max_len, len(vocab)]\n",
    "                    img = img.to(model.device)\n",
    "                    tokens = tokens.to(model.device)\n",
    "                    output = model(img)\n",
    "  \n",
    "\n",
    "                    # reshape target and output for loss\n",
    "                    # output_flat [(max_len - 1) * batch_size, len(vocab)]\n",
    "                    # target_flat [(max_len - 1) * batch_size]\n",
    "                    output_dim = output.shape[-1]\n",
    "                    output_flat = output[:, 1:].view(-1, output_dim)\n",
    "                    tokens_flat = tokens[:, 1:].view(-1)\n",
    "\n",
    "                    loss = loss_fn(output_flat, tokens_flat)\n",
    "\n",
    "                    # will raise error with no grads\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    # scale loss by length of dataloader to obtain avg loss\n",
    "                    losses[phase] += loss.item() / lengths[phase]\n",
    "\n",
    "                    # [TODO] Mb ignore padding ???\n",
    "                    predictions = output.argmax(2)\n",
    "\n",
    "                    # get index of <EOS> token guaranteed to be only one in tokens\n",
    "                    end_token_idx = (tokens == end_token).nonzero(\n",
    "                        as_tuple=True\n",
    "                    )[-1]\n",
    "                    # truncate to only valuable output\n",
    "                    # after hitting EOS tokens all true tokens will be <PAD>\n",
    "                    predictions = predictions[:, 1:end_token_idx]\n",
    "                    tokens = tokens[:, 1:end_token_idx]\n",
    "\n",
    "                    # compute accuracy\n",
    "                    num_correct[phase] += torch.sum(\n",
    "                        predictions == tokens,\n",
    "                        dim=1,\n",
    "                    ).item()\n",
    "\n",
    "                    num_total[phase] += tokens.size(0) * tokens.size(1)\n",
    "                    accuracies[phase] = num_correct[phase] / num_total[phase]\n",
    "\n",
    "                    # compute edit distances over batch\n",
    "                    for b in range(tokens.size(0)):\n",
    "                        total_distances[phase] += edit_distance(\n",
    "                            predictions[b], tokens[b]\n",
    "                        )\n",
    "                    # average over epoch\n",
    "                    levenstein[phase] = (\n",
    "                        total_distances[phase]\n",
    "                        # number of sequences processed = total_tokens / seq_len\n",
    "                        / (num_total[phase] / tokens.size(1))\n",
    "                    )\n",
    "\n",
    "                    # make progress string\n",
    "                    report = make_training_report(\n",
    "                        losses[phase],\n",
    "                        levenstein[phase],\n",
    "                        accuracies[phase],\n",
    "                        scheduler.get_last_lr()[0],\n",
    "                    )\n",
    "                    pbar.set_postfix_str(report)\n",
    "\n",
    "                # step scheduler after epoch if training\n",
    "                if phase == \"train\":\n",
    "                    lr = scheduler.get_last_lr()[0]\n",
    "                    scheduler.step()\n",
    "\n",
    "            # save training history\n",
    "            history[epoch] = {\n",
    "                \"acc\": accuracies,\n",
    "                \"edit_dist\": levenstein,\n",
    "                \"loss\": losses,\n",
    "                \"lr\": lr,\n",
    "            }\n",
    "\n",
    "            if losses[\"val\"] < min_loss:\n",
    "                min_loss = losses[\"val\"]\n",
    "                file_path = f\"{checkpoint}/{datetime.now().strftime('%d%m-%H%M%S')}-acc-{accuracies['val']:.3f}.pth\"\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"model\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"scheduler\": scheduler.state_dict(),\n",
    "                        \"history\": history,\n",
    "                    },\n",
    "                    file_path,\n",
    "                )\n",
    "\n",
    "                _last_saved.append(file_path)\n",
    "\n",
    "                # remove file if more than KEEP is stored\n",
    "                if len(_last_saved) > keep:\n",
    "                    oldest = _last_saved.pop(0)\n",
    "\n",
    "                    if os.path.exists(oldest):\n",
    "                        os.remove(oldest)\n",
    "\n",
    "    print(f\"Last checkpoint: {_last_saved[-1]}\")\n",
    "    return history, model\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    # model_arch: str,\n",
    "    checkpoint: str,\n",
    "    data: list[pd.DataFrame],\n",
    "    batch_size: int,\n",
    "    num_epochs: int,\n",
    "    device: torch.device,\n",
    "    vocab: dict[str, int],\n",
    "    image_folder: str,\n",
    "):\n",
    "    # [TODO] do not pass vocab only pass vocab length\n",
    "\n",
    "    model = CNNLSTM(vocab_len=len(vocab), device=device)\n",
    "\n",
    "    if checkpoint == \"NA\":\n",
    "        checkpoint = f\"./artifacts/{CNNLSTM.__name__}/\"\n",
    "\n",
    "    if not os.path.exists(checkpoint):\n",
    "        os.mkdir(checkpoint)\n",
    "\n",
    "    # if batch_size is not specified\n",
    "    if batch_size == 0:\n",
    "        batch_size = 1 if device == \"cpu\" else 8\n",
    "\n",
    "    # [TODO] Use model.input_size member\n",
    "    transforms = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Grayscale(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    phases = [\"train\", \"val\", \"test\"]\n",
    "    datasets = {\n",
    "        phase: IM2LaTEX100K(\n",
    "            data=data,\n",
    "            transform=transforms,\n",
    "            vocab_len=len(vocab),\n",
    "            image_folder=image_folder,\n",
    "        )\n",
    "        for phase, data in zip(phases, data)\n",
    "    }\n",
    "    dataloaders = {\n",
    "        phase: DataLoader(dataset, batch_size=batch_size)\n",
    "        for phase, dataset in datasets.items()\n",
    "    }\n",
    "\n",
    "    ce_loss = nn.CrossEntropyLoss(ignore_index=vocab[\"<PAD>\"])\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=5e-2)\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer=optimizer, T_max=10, eta_min=1e-3, last_epoch=-1\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    history, model = training_func(\n",
    "                        model=model,\n",
    "                        n_epochs=num_epochs,\n",
    "                        loss_fn=ce_loss,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        dataloaders=dataloaders,\n",
    "                        checkpoint=checkpoint,\n",
    "                        end_token=vocab[\"<EOS>\"],\n",
    "                    )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape=(102863, 1)\n",
      "df.shape=(5, 2)\n",
      "df.shape=(5, 2)\n",
      "df.shape=(5, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train(001): 100%|██████████| 5/5 [00:35<00:00,  7.20s/batch, Loss=12.7653 | Acc=0.0478 | ED=86.7 | LR=5.0e-02]\n",
      "Val  (001): 100%|██████████| 5/5 [00:11<00:00,  2.22s/batch, Loss=14.3147 | Acc=0.0533 | ED=86.1 | LR=4.9e-02]\n",
      "Test (001):   0%|          | 0/5 [00:02<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m val_df \u001b[39m=\u001b[39m preprocess_function(input_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../data/interim/im2latex.debug.csv\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m                                vocab\u001b[39m=\u001b[39mtoken2id,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m                                col_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mformula\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m                                add_padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m                                debug\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m test_df \u001b[39m=\u001b[39m preprocess_function(input_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../data/interim/im2latex.debug.csv\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m                                vocab\u001b[39m=\u001b[39mtoken2id,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m                                col_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mformula\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m                                add_padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m                                debug\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m history, model \u001b[39m=\u001b[39m train_model(checkpoint\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../artifacts/CNNLSTM/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m                              data\u001b[39m=\u001b[39;49m[train_df, val_df, test_df],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m                              batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m                              num_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m                              device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m                              vocab\u001b[39m=\u001b[39;49mtoken2id,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m                              image_folder\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../data/raw/formula_images_processed/formula_images_processed/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m                              )\n",
      "\u001b[1;32m/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=206'>207</a>\u001b[0m scheduler \u001b[39m=\u001b[39m CosineAnnealingLR(\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=207'>208</a>\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer, T_max\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, eta_min\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, last_epoch\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=208'>209</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=210'>211</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=212'>213</a>\u001b[0m history, model \u001b[39m=\u001b[39m training_func(\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=213'>214</a>\u001b[0m                     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=214'>215</a>\u001b[0m                     n_epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=215'>216</a>\u001b[0m                     loss_fn\u001b[39m=\u001b[39;49mce_loss,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=216'>217</a>\u001b[0m                     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=217'>218</a>\u001b[0m                     scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=218'>219</a>\u001b[0m                     dataloaders\u001b[39m=\u001b[39;49mdataloaders,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=219'>220</a>\u001b[0m                     checkpoint\u001b[39m=\u001b[39;49mcheckpoint,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=220'>221</a>\u001b[0m                     end_token\u001b[39m=\u001b[39;49mvocab[\u001b[39m\"\u001b[39;49m\u001b[39m<EOS>\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=221'>222</a>\u001b[0m                 )\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=223'>224</a>\u001b[0m \u001b[39mreturn\u001b[39;00m history, model\n",
      "\u001b[1;32m/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39m# compute edit distances over batch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(tokens\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)):\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m     total_distances[phase] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m edit_distance(\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m         predictions[b], tokens[b]\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39m# average over epoch\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m levenstein[phase] \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     total_distances[phase]\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m     \u001b[39m# number of sequences processed = total_tokens / seq_len\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39m/\u001b[39m (num_total[phase] \u001b[39m/\u001b[39m tokens\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m )\n",
      "\u001b[1;32m/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m t1 \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, len1 \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t2 \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, len1 \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39mif\u001b[39;00m token1[t1 \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m token2[t2 \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m             distances[t1][t2] \u001b[39m=\u001b[39m distances[t1 \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m][t2 \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/zeinovich/projects/img2latex/notebooks/v0.1-zeinovich-colab-training.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "token2id, _ = make_vocab_func(input_file=\"../data/raw/im2latex_formulas.norm.csv\",\n",
    "                              col_name=\"formulas\",\n",
    "                              add_special=True)\n",
    "\n",
    "train_df = preprocess_function(input_file=\"../data/interim/im2latex.debug.csv\",\n",
    "                               vocab=token2id,\n",
    "                               col_name=\"formula\",\n",
    "                               add_padding=True,\n",
    "                               debug=False)\n",
    "val_df = preprocess_function(input_file=\"../data/interim/im2latex.debug.csv\",\n",
    "                               vocab=token2id,\n",
    "                               col_name=\"formula\",\n",
    "                               add_padding=True,\n",
    "                               debug=False)\n",
    "test_df = preprocess_function(input_file=\"../data/interim/im2latex.debug.csv\",\n",
    "                               vocab=token2id,\n",
    "                               col_name=\"formula\",\n",
    "                               add_padding=True,\n",
    "                               debug=False)\n",
    "\n",
    "history, model = train_model(checkpoint=\"../artifacts/CNNLSTM/\",\n",
    "                             data=[train_df, val_df, test_df],\n",
    "                             batch_size=1,\n",
    "                             num_epochs=1,\n",
    "                             device=\"cpu\",\n",
    "                             vocab=token2id,\n",
    "                             image_folder=\"../data/raw/formula_images_processed/formula_images_processed/\",\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
